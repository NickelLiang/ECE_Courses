{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10: Central limit theorem, change detection, multidimensional Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as st\n",
    "from scipy.stats import multivariate_normal\n",
    "print 'Modules Imported!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Distribution and the Central Limit Theorem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaussian distribution (also known as the normal distribution) is a continuous type distribution and has a pdf defined by $f(u)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(\\frac{(u-\\mu)^2}{2\\sigma^2}\\right)$. The mean is given by $\\mu$ and the variance is given by $\\sigma^2$. Below is a graph of the pdf and the CDF of the standard Gaussian ($\\mu=0, \\sigma^2=1$). As shown in your probability course, the CDF is simply the integral of the pmf. Let $X \\sim Gauss(0,1)$. $P\\{X\\le c\\}=\\Phi(c)=\\int^c_{-\\infty} f(u)\\,du$ This is known as the Phi function, but often the complementary CDF, or Q function, is used. $Q(c)=P\\{X\\ge c\\}=\\int^{\\infty}_c f(u)\\,du = 1-\\Phi(c) = \\Phi(-c)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = st.norm()\n",
    "x = np.linspace(-10,10,1000)\n",
    "plt.plot(x,X.pdf(x))\n",
    "plt.title('pdf of standard Gaussian')\n",
    "plt.figure()\n",
    "plt.plot(x,X.cdf(x))\n",
    "plt.title('CDF of standard Gaussian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also shift and stretch the Gaussian. Notice how the scaling changes when we change the mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = st.norm(3,4)\n",
    "x = np.linspace(-10,10,1000)\n",
    "plt.plot(x,X.pdf(x))\n",
    "plt.title('pdf of N(3,4) Gaussian')\n",
    "plt.figure()\n",
    "plt.plot(x,X.cdf(x))\n",
    "plt.title('CDF of N(3,4) Gaussian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaussian distribution is one of the most frequently used distributions due to the central limit theorem (CLT).  To discuss the CLT, we begin with the law of large numbers (LLN).  The LLN, roughly speaking, tells us that if $X_1, X_2, \\cdots $ is a sequence of independent and identically distributed random variables with mean $\\mu$, and if $S_n =X_1+X_2+\\cdots+X_n,$  then, with probability one, $\\lim_{n\\to\\infty}\\frac{S_n}{n}=\\mu$.   This gives rise to the practical approximation, $S_n \\approx n \\mu.$   For example, if we roll a fair die 1000 times, the sum of the numbers rolled should be approximately  3,500.   \n",
    "\n",
    "The CLT gives an important refinement on the LLN.   Roughly speaking, it tells us that $S_n$ as just described tends to have a Gaussian distribution.  If each $X_k$ has mean $\\mu$ and variance $\\sigma^2,$  then $S_n$ has mean $n\\mu$ and variance $n\\sigma^2.$   Therefore, the standardized version of $S_n$ is $\\frac{S_n-n\\mu}{\\sqrt{n\\sigma^2}}.$  The CLT states that for any constant $c,$\n",
    "$$\n",
    "\\lim_{n\\to\\infty}  P\\left\\{  \\frac{S_n-n\\mu}{\\sqrt{n\\sigma^2}} \\leq c \\right\\} = \\Phi(c)\n",
    "$$\n",
    "In practice, this gives the Gaussian approximation: $S_n$ approximately has the\n",
    "Gaussian (same as normal) distribution with parameters $n\\mu$ and\n",
    "variance $n\\sigma^2.$\n",
    "\n",
    "\n",
    "In order to visualize this, let's look at sums of Bernoulli random variables. Suppose we have $n$ indpendent Bernoulli random variables, $X_1,X_2,\\cdots,X_n$, each with parameter $p$. Recall that the Bernoulli distribution has a mean of $\\mu_X=p$ and a variance of $\\sigma_X^2=p(1-p)$.  The sum of these random variables, of course, has the the binomial distribution with parameters $n$ and $p$. That is, $S_n=(X_1+X_2+\\cdots+X_n) \\sim Bin(n,p)$. If we standardize our binomial (using $\\mu = np, \\sigma^2 = np(1-p)$) we find the following: \n",
    "\n",
    "$ \\frac{S_n-np}{\\sqrt{np(1-p)}}=\\frac{S_n-np}{\\sqrt{n}\\sqrt{p(1-p)}} = \\frac{S_n-n\\mu_X}{\\sqrt{n}\\sigma_X}$\n",
    "\n",
    "By the central limit theorem, the distribution of this goes to the standard normal distribution as n goes to infinity.  (This was the first example of the CLT discovered, and is called the DeMoivre-Laplace limit theorem.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**<SPAN style=\"BACKGROUND-COLOR: #C0C0C0\">Problem 1:</SPAN>** Show graphically that as $n$ becomes large, the distribution of the sum of $n$ i.i.d. Bernoulli random variables converges to the normal distribution. To do this use n = 50 and $p = 0.4.$ \n",
    "\n",
    "1). Overlay a plot of the pmf of the binomial distribution versus the pdf of a normal distribution with the same mean and variance. Your pmf should be discrete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2). Overlay a plot of the CDF of the binomial distribution versus the CDF of a normal distribution with the same mean and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3). Comment on what happens as you change $n.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__ (Your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<SPAN style=\"BACKGROUND-COLOR: #C0C0C0\">End of Problem 1</SPAN>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to view the central limit theorem is through statistics. Suppose we have any discrete distribution. For instance, let's go back to our apocalyptic zombie scenario from Lab 6. Recall that the pmf of the number of zombies getting into a building in a given night has  the distribution:\n",
    "\n",
    "$P\\{Z = 5\\} = .05$\n",
    "\n",
    "$P\\{Z = 3\\} = .1$\n",
    "\n",
    "$P\\{Z = 2\\} = .25$\n",
    "\n",
    "$P\\{Z = 1\\} = .2$\n",
    "\n",
    "$P\\{Z = 0\\} = .05$\n",
    "\n",
    "$P\\{Z = -2\\} = .2$\n",
    "\n",
    "$P\\{Z = -3\\} = .1$\n",
    "\n",
    "$P\\{Z = -4\\} = .05$\n",
    "\n",
    "We're assuming that this pmf is the same each night. Suppose that an anti-Zombie coalition has been formed across campus and includes 150 buildings (all with the same distribution). One of the survivors just happens to be a statistician who wants to assess the campus's survival capability. He goes to each building each night for twenty nights, and observes how many zombies enter.  For each building he calculates the average number of Zombies per night that he saw enter the building.  This results in 150 averages of 20 random variates each. \n",
    "\n",
    "<br>**<SPAN style=\"BACKGROUND-COLOR: #C0C0C0\">Problem 2:</SPAN>** \n",
    "\n",
    "1. Create a histogram of the averages across the buildings with the number of bins being equal to the square root of the number of buildings. \n",
    "2. Answer the following questions: Is your histogram approximately normally distributed? What happens as you increase the number of buildings? What happens as you increase the number of nights observed? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__: (Your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<SPAN style=\"BACKGROUND-COLOR: #C0C0C0\">End of Problem 2</SPAN>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Detection:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often complex systems or machines have sensors to monitor the health of the machine.  The sensor outputs might be modeled as iid with some pmf $p_o$ as long as the system is in good condition,  and iid with some other pmf $p_1$ if the system has changed in some way (such as system failure, intruder present, etc).    A detection rule observes the data and raises the alarm at some time $\\tau.$  Ideally the alarm time $\\tau$ is always greater than, but not much greater than, the system change time. One approach to this problem is to fix a window length $W$ and divide time into a sequence of nonoverlapping time windows.  At the end of each window we perform a binary hypothesis test to decide if the data in the window was generated by $p_0$\n",
    "or $p_1.$  If the decision is to decide in favor of $p_1$ the alarm is raised, so that $\\tau$ is the time at the end of the window.   This scenario is simulated below.   Try running the simulation muliple times.  Try experimenting by varying the detection threshold and window size.  You might notice that sometimes the log likelihood ratio crosses above the threshold in the middle of a window, but the alarm isn't sounded because the likelihood ratio is tested only at the end of a window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Window method for change detection\n",
    "N_max=1000 # maximum number of observations allowed\n",
    "gamma=np.random.randint(0,700)  # time of system change\n",
    "W=30  # window length, initally W=30\n",
    "threshold=5.0   # detection threshold, initally 5.0\n",
    "p0=np.array([0.2,0.2,0.4,0.2])\n",
    "p1=np.array([0.4,0.3,0.2,0.1])\n",
    "if np.size(p0)!=np.size(p1):\n",
    "    print \"warning, p0 and p1 have different sizes\"\n",
    "\n",
    "# Observations will have pmf p0 for times 1 through gamma - 1, pmf p1 afterwards\n",
    "def f(i):\n",
    "    return np.log(p1[i]/p0[i])\n",
    "\n",
    "c=np.arange(np.size(p0))\n",
    "Xcstm0 = st.rv_discrete(values=(c,p0))   # scipy.stats object for distibution p0\n",
    "Xcstm1 = st.rv_discrete(values=(c,p1))   # scipy.stats object for distibution p1\n",
    "\n",
    "variates=np.column_stack([Xcstm0.rvs(size=N_max),Xcstm1.rvs(size=N_max)])  #Nmax x 2 array\n",
    "log_LR=np.zeros(N_max+1)  # log_LR will store the sequence of log likelihood ratios\n",
    "\n",
    "t=0\n",
    "alarm_flag=0\n",
    "while (t<N_max-1):\n",
    "    t=t+1\n",
    "    if t<gamma:\n",
    "        log_LR[t]=log_LR[t-1]+f(variates[t,0])\n",
    "    else:\n",
    "        log_LR[t]=log_LR[t-1]+f(variates[t,1])                             \n",
    "    \n",
    "    if t % W==0:    # if t is a multiple of W, time to do an LRT\n",
    "        if log_LR[t] > threshold:\n",
    "            alarm_flag=1\n",
    "            alarm_time=t\n",
    "            break\n",
    "        else:    # Reset LR\n",
    "            log_LR[t]=0.\n",
    "            \n",
    "print \"Window Size=\",W,\"LRT threshold=\",threshold\n",
    "\n",
    "if alarm_flag==0:\n",
    "    print \"Time N_max reached with no alarm\"\n",
    "else:\n",
    "    if (alarm_time < gamma):\n",
    "        print \"False alarm at time\", alarm_time\n",
    "    else:\n",
    "        print \"System change detected with time to detection\", alarm_time-gamma\n",
    "        \n",
    "plt.plot(log_LR[0:alarm_time+50])\n",
    "plt.plot(gamma,0,'ro')   # Time of system change indicated by red dot\n",
    "plt.plot(alarm_time,0,'go')   # Alarm time indicated by green dot\n",
    "plt.title('cumulative log likelihood ratio within windows vs. time')\n",
    "plt.ylabel('log likelihood')\n",
    "plt.xlabel('time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**<SPAN style=\"BACKGROUND-COLOR: #C0C0C0\">Problem 3:</SPAN>** Run the above simulation 1,000 times. Calculate and print out: \n",
    "\n",
    "1. the experimental probability of a false alarm\n",
    "2. the mean time to detection given that the false alarm does not happen. \n",
    "\n",
    "If failure is not detected use $N_{max}-\\gamma$ for detection time. (Again, it's probably in your best interest not to plot a graph for each trial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__: (Your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<SPAN style=\"BACKGROUND-COLOR: #C0C0C0\">End of Problem 3</SPAN>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Longer window sizes in the above method of change detection can lead to more accurate hypothesis testing (reducing the probability of false alarm and/or increasing the probability of detection during a given window after system change occurs), but longer window sizes can also lead to larger time to detection because after the system change we have to wait at least until the next window boundary (or the one after that) to get a detection.   An alternative method, called the *cumulative sum* method, is to continUally update the log likelihood ratio, but reseting it to zero whenever it goes negative, and sounding the alarm whenever it crosses above a threshold.  Note that a somewhat larger threshold should be used for the cumulative sum algorithm to offset the fact that the negative values of log likelihood are bumped up to zero.\n",
    "\n",
    "<br>**<SPAN style=\"BACKGROUND-COLOR: #C0C0C0\">Problem 4:</SPAN>** \n",
    "\n",
    "1. Implement the cumulative sum algorithm for the same pair of distributions and same distribution of system change time $\\gamma$ as above.   Adjust the threshold for the cumulative sum algorithm to get approximately the same probability of false alarm as for the window method above (this may require some trial and error). \n",
    "2. Print out the probability of false alarm. Estimate the resulting mean time to detection and print it out.  \n",
    "3. Comment on how it differs from the average we found above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__ (Your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<SPAN style=\"BACKGROUND-COLOR: #C0C0C0\">End of Problem 4</SPAN>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Hypothesis Testing for Multidimensional Gaussian Distributions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ECE 313 we consider the bivariate Gaussian distribution.  It is a joint distribution for two random variables, $X_1,X_2$ and is uniquely determined by five parameters, the means of the two random variables, $m_1$ and $m_2$, the variances of the two random variables, and the covariance between the two random variables defined by $\\mbox{Cov}(X_1,X_2)=E[(X_1-m_1)(X_2-m_2)].$ \n",
    "By the way, note that $\\mbox{Cov}(X_1,X_1)=\\mbox{Var}(X_1,X_1).$   Equivalently,\n",
    "we can think of $\\binom{X_1}{X_2}$ as a random vector, with mean $\\binom{m_1}{m_2}$ and covariance matrix\n",
    "$\\Sigma=\\left( \\begin{array}  \\mbox{Cov}(X_1,X_1) & \\mbox{Cov}(X_1,X_2)\\\\ \\mbox{Cov}(X_2,X_1) & \\mbox{Cov}(X_2,X_2) \\end{array}\\right).$\n",
    "Joint normal (also known as joint Gaussian) distributions exist in any number of dimensions.   A Guassian distribution in a given number of dimensions is specified uniquely by a mean vector and a covariance matrix.  The following code generates variates for two normal distributions.  The orange triangles follow a distribution that is rotationally symmetric about the origin.   The blue circles follow a distribution with positive correlation between the two coordinates; the shape of the blue blob of points is elongated along a line of slope one.  Also, the mean vector for the blue points is $\\binom{2.0}{0}$ so the blue blob is offset a bit to the right of the orange blob.   Try running the code a few times to see the variation.   To get a better idea of the shapes, try increasing the number of samples to 1000.   Now suppose you were to have 50 samples generated from one of the two distributions. That is, you get to see either 50 orange points or 50 blue points, but with the colors removed.  How well do you think the maximum likelihood decision rule could detect which distribution was used to generate the points?  (This is stated as a problem for you to work out,  below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dim=2  # Dimension of the random vectors\n",
    "num_samples=200\n",
    "Sigma0=2.0*np.identity(dim)    # identity matrix\n",
    "Sigma1=np.identity(dim)+ 4.0*np.ones([dim,dim])   # some positive correlation added\n",
    "mu0=np.zeros(dim)\n",
    "mu1=np.zeros(dim)\n",
    "mu1[0]=2.0  # first coordinate has nonzero mean under H1\n",
    "variates0= multivariate_normal.rvs(mu0,Sigma0,num_samples)\n",
    "variates1= multivariate_normal.rvs(mu1,Sigma1,num_samples)\n",
    "plt.scatter(variates0[:,0],variates0[:,1],color='orange',marker='^')\n",
    "plt.scatter(variates1[:,0],variates1[:,1],color='blue')\n",
    "# plt.plot.scatter(variates2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code runs a ML detection rule on simulated multidimensional Gaussian random vectors of any finite dimension. It is very similar to the code used for hypothesis testing at the beginning of Lab 9.  The difference is that here two multivariate normal pdfs are used instead of two discrete distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#   Simulation of ML detection rule for two multidimensional Gaussian distibutions\n",
    "#\n",
    "\n",
    "dim = 3  # Dimension of the random vectors\n",
    "num_samples = 10\n",
    "Sigma0 = np.identity(dim)    # identity matrix\n",
    "Sigma1 = np.identity(dim)+ 0.5*np.ones([dim,dim])   # some positive correlation added\n",
    "mu0 = np.zeros(dim)\n",
    "mu1 = np.ones(dim)*0.1   #  small nonzero mean under H1\n",
    "\n",
    "dist0 = multivariate_normal(mu0, Sigma0) # multivariate_normal was imported from Scipy\n",
    "dist1 = multivariate_normal(mu1, Sigma1)\n",
    "\n",
    "Htrue=np.random.randint(2)  # Sets the true hypothesis to be 0 or 1.\n",
    "\n",
    "if Htrue==0:     # generate num_samples random variates using the true hypothesis\n",
    "    variates = dist0.rvs(num_samples) # num_samples x dim array, each row is random variate   \n",
    "else:\n",
    "    variates = dist1.rvs(num_samples)   # num_samples x dim array, each row is random variate\n",
    "\n",
    "print \"Data is generated using true hypothesis H\",Htrue ,\":  \"\n",
    "print variates\n",
    "\n",
    "log_LR=0.0   # log_LR will become the log likelihood ratio \n",
    "for count in range(num_samples):\n",
    "    log_LR += np.log(dist1.pdf(variates[count,:])/dist0.pdf(variates[count,:]))\n",
    "    \n",
    "if log_LR >= 0:\n",
    "    print \"log_LR=\", log_LR, \">=0; declare H1 is true\"\n",
    "else: print \"log_LR=\", log_LR, \"<0; declare H0 is true\"\n",
    "\n",
    "if (log_LR >=0) and (Htrue==0):\n",
    "    print \"False Alarm occured\"\n",
    "if (log_LR <0) and (Htrue==1):\n",
    "    print \"Miss occured\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**<SPAN style=\"BACKGROUND-COLOR: #C0C0C0\">Problem 5:</SPAN>** Adapt the above code to the case of 50 samples of bivariate gaussian random variable using the parameters of the orange and blue scatter plots shown above. Run the simulation 1,000 times to estimate and print out the probability of a false alarm and the probability of a miss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__ (Your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<SPAN style=\"BACKGROUND-COLOR: #C0C0C0\">End of Problem 5</SPAN>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this weeks lab, please answer all questions 1-5."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
